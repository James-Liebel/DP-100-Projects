{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Deep Learning with TensorFlow\n",
       "\n",
       "Classical machine learning relies on using statistics to determine relationships between features and labels, and can be very effective for creating predictive models. However, a massive growth in the availability of data coupled with advances in the computing technology required to process it has led to the emergence of new machine learning techniques that mimic the way the brain processes information in a structure called an artificial neural network.\n",
       "\n",
       "TensorFlow is a framework for creating machine learning models, including deep neural networks (DNNs). In this example, we'll use Tensorflow to create a simple neural network that classifies penguins into species based on the length and depth of their culmen (bill), their flipper length, and their body mass.\n",
       "\n",
       "> **Citation**: The penguins dataset used in this the exercise is a subset of data collected and made available by [Dr.Â Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station, Antarctica LTER](https://pal.lternet.edu/), a member of the [Long Term Ecological Research Network](https://lternet.edu/).\n",
       "\n",
       "## Explore the dataset\n",
       "\n",
       "Before we start using TensorFlow to create a model, let's load the data we need from the Palmer Islands penguins dataset, which contains observations of three different species of penguin.\n",
       "\n",
       "> **Note**: In reality, you can solve the penguin classification problem easily using classical machine learning techniques without the need for a deep learning model; but it's a useful, easy to understand dataset with which to demonstrate the principles of neural networks in this notebook."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "import pandas as pd\n",
       "\n",
       "# load the training dataset (excluding rows with null values)\n",
       "penguins = pd.read_csv('data/penguins.csv').dropna()\n",
       "\n",
       "# Deep Learning models work best when features are on similar scales\n",
       "penguins['FlipperLength'] = penguins['FlipperLength']/10\n",
       "penguins['BodyMass'] = penguins['BodyMass']/100\n",
       "\n",
       "# Oversample dataset to make it larger for deep learning\n",
       "for i in range(1,3):\n",
       "    penguins = penguins.append(penguins)\n",
       "\n",
       "# Display a random sample of 10 observations\n",
       "sample = penguins.sample(10)\n",
       "sample"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "The **Species** column is the label our model will predict. Each label value represents a class of penguin species, encoded as 0, 1, or 2. The following code shows the actual species to which these class labels correspond."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "penguin_classes = ['Adelie', 'Gentoo', 'Chinstrap']\n",
       "print(sample.columns[0:5].values, 'SpeciesName')\n",
       "for index, row in penguins.sample(10).iterrows():\n",
       "    print('[',row[0], row[1], row[2],row[3], int(row[4]), ']',penguin_classes[int(row[-1])])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "As is common in a supervised learning problem, we'll split the dataset into a set of records with which to train the model, and a smaller set with which to validate the trained model."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "from sklearn.model_selection import train_test_split\n",
       "\n",
       "features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
       "label = 'Species'\n",
       "\n",
       "# Split data 70%-30% into training set and test set\n",
       "x_train, x_test, y_train, y_test = train_test_split(penguins[features].values,\n",
       "                                                    penguins[label].values,\n",
       "                                                    test_size=0.30,\n",
       "                                                    random_state=0)\n",
       "\n",
       "print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n",
       "print(\"Sample of features and labels:\")\n",
       "for n in range(0,24):\n",
       "    print(x_train[n], y_train[n], '(' + penguin_classes[y_train[n]] + ')')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "The *features* are the measurements for each penguin observation, and the *label* is a numeric value that indicates the species of penguin that the observation represents (Adelie, Gentoo, or Chinstrap).\n",
       "\n",
       "## Install and import TensorFlow libraries\n",
       "\n",
       "Since we plan to use TensorFlow to create our penguin classifier, we'll need to install and import the libraries we intend to use.\n",
       "\n",
       "> **Note** *Keras* is an abstraction layer over the base TensorFlow API."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "!pip install --upgrade tensorflow"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "import tensorflow\n",
       "from tensorflow import keras\n",
       "from tensorflow.keras import models, utils, optimizers\n",
       "from tensorflow.keras.models import Sequential\n",
       "from tensorflow.keras.layers import Dense\n",
       "\n",
       "tensorflow.random.set_seed(0)\n",
       "print(\"Libraries imported.\")\n",
       "print('Keras version:',keras.__version__)\n",
       "print('TensorFlow version:',tensorflow.__version__)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Prepare the data for TensorFlow\n",
       "\n",
       "Convert features to 32-bit floats and labels to categorical format."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "x_train = x_train.astype('float32')\n",
       "x_test = x_test.astype('float32')\n",
       "y_train = utils.to_categorical(y_train)\n",
       "y_test = utils.to_categorical(y_test)\n",
       "print('Ready...')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Define a neural network\n",
       "\n",
       "Create a 3-layer fully connected network."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "hl = 10  # hidden layer nodes\n",
       "model = Sequential()\n",
       "model.add(Dense(hl, input_dim=len(features), activation='relu'))\n",
       "model.add(Dense(hl, input_dim=hl, activation='relu'))\n",
       "model.add(Dense(len(penguin_classes), input_dim=hl, activation='softmax'))\n",
       "print(model.summary())"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Train the model\n",
       "\n",
       "Use Adam optimizer and categorical cross-entropy loss for 50 epochs."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "learning_rate = 0.001\n",
       "opt = optimizers.Adam(lr=learning_rate)\n",
       "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
       "num_epochs = 50\n",
       "history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=10, validation_data=(x_test, y_test))"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Plot training and validation loss"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "%matplotlib inline\n",
       "from matplotlib import pyplot as plt\n",
       "epoch_nums = range(1,num_epochs+1)\n",
       "plt.plot(epoch_nums, history.history[\"loss\"])\n",
       "plt.plot(epoch_nums, history.history[\"val_loss\"])\n",
       "plt.xlabel('epoch')\n",
       "plt.ylabel('loss')\n",
       "plt.legend(['training','validation'], loc='upper right')\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Save and use the trained model"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "modelFileName = 'models/penguin-classifier.h5'\n",
       "model.save(modelFileName)\n",
       "del model\n",
       "print('model saved as', modelFileName)"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": [
       "from tensorflow.keras import models\n",
       "import numpy as np\n",
       "model = models.load_model(modelFileName)\n",
       "x_new = np.array([[50.4,15.3,20,50]])\n",
       "class_probabilities = model.predict(x_new)\n",
       "predictions = np.argmax(class_probabilities, axis=1)\n",
       "print(predictions)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
     },
     "language_info": {
      "name": "python",
      "version": "3.6.9"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   